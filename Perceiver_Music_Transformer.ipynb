{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
          "kernelId": ""
        },
        "id": "SiTIpPjArIyr"
      },
      "source": [
        "# Perceiver Music Transformer (ver. 1.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2023\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GPU CHECK)"
      ],
      "metadata": {
        "id": "0r3o-gVUb_en"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "39411b40-9e39-416e-8fe4-d40f733e7956",
          "kernelId": ""
        },
        "id": "lw-4aqV3sKQG"
      },
      "outputs": [],
      "source": [
        "#@title NVIDIA GPU Check\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
          "kernelId": ""
        },
        "id": "gOd93yV0sGd2"
      },
      "source": [
        "# (SETUP ENVIRONMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
          "kernelId": ""
        },
        "id": "fX12Yquyuihc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install all dependencies (run only once per session)\n",
        "\n",
        "!git clone https://github.com/asigalov61/Perceiver-Music-Transformer\n",
        "!pip install einops\n",
        "!pip install torch\n",
        "!pip install torch-summary\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
          "kernelId": ""
        },
        "id": "z7n9vnKmug1J"
      },
      "outputs": [],
      "source": [
        "#@title Import all needed modules\n",
        "\n",
        "print('Loading needed modules. Please wait...')\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "print('Loading core modules...')\n",
        "os.chdir('/content/Perceiver-Music-Transformer')\n",
        "\n",
        "import TMIDIX\n",
        "\n",
        "from perceiver_ar_pytorch_full import PerceiverAR, AutoregressiveWrapper\n",
        "\n",
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "os.chdir('/content/')\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (UNZIP MODEL)"
      ],
      "metadata": {
        "id": "iJ4xaSoA9A9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip Pre-Trained Perceiver Music Transformer Model\n",
        "print('=' * 70)\n",
        "%cd /content/Perceiver-Music-Transformer/Model\n",
        "\n",
        "print('=' * 70)\n",
        "print('Unzipping pre-trained Perceiver Music Transformer model...Please wait...')\n",
        "\n",
        "!cat /content/Perceiver-Music-Transformer/Model/Perceiver_Music_Transformer_Trained_Model.zip* > /content/Perceiver-Music-Transformer/Model/Perceiver_Music_Transformer_Trained_Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "!unzip -j /content/Perceiver-Music-Transformer/Model/Perceiver_Music_Transformer_Trained_Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "print('Done! Enjoy! :)')\n",
        "print('=' * 70)\n",
        "%cd /content/\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "dICiWco0JwU4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKFoeke9L7H"
      },
      "source": [
        "# (LOAD MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load/Reload the model\n",
        "\n",
        "full_path_to_model_checkpoint = \"/content/Perceiver-Music-Transformer/Model/Perceiver_Music_Transformer_Trained_Model_16026_steps_0.8256_loss.pth\" #@param {type:\"string\"}\n",
        "\n",
        "print('Loading the model...')\n",
        "# Load model\n",
        "\n",
        "# constants\n",
        "\n",
        "SEQ_LEN = 6144 # 6k\n",
        "PREFIX_SEQ_LEN = 4096 # 4k\n",
        "\n",
        "model = PerceiverAR(\n",
        "    num_tokens = 2145,\n",
        "    dim = 1024,\n",
        "    depth = 32,\n",
        "    ff_mult=2,\n",
        "    cross_attn_dropout = 0.25,\n",
        "    max_seq_len = SEQ_LEN,\n",
        "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
        ")\n",
        "\n",
        "model = AutoregressiveWrapper(model)\n",
        "\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "state_dict = torch.load(full_path_to_model_checkpoint)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "# Model stats\n",
        "summary(model)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-NLe35B0b9a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GENERATE)"
      ],
      "metadata": {
        "id": "RCKJSbl4erb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Seed/Custom MIDI\n",
        "\n",
        "#@markdown PLEASE NOTE: Custom MIDI must have at least 1024 notes to fill-in Perceiver prefix requirements.\n",
        "\n",
        "full_path_to_custom_MIDI_file = \"/content/Perceiver-Music-Transformer/Seeds/Perceiver-Music-Transformer-Sample-Piano-Seed-MIDI.mid\" #@param {type:\"string\"}\n",
        "\n",
        "print('Loading custom MIDI file...')\n",
        "score = TMIDIX.midi2ms_score(open(full_path_to_custom_MIDI_file, 'rb').read())\n",
        "\n",
        "events_matrix = []\n",
        "\n",
        "itrack = 1\n",
        "\n",
        "#==================================================\n",
        "\n",
        "patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "patch_map = [\n",
        "            [0, 1, 2, 3, 4, 5, 6, 7], # Piano \n",
        "            [24, 25, 26, 27, 28, 29, 30], # Guitar\n",
        "            [32, 33, 34, 35, 36, 37, 38, 39], # Bass\n",
        "            [40, 41], # Violin\n",
        "            [42, 43], # Cello\n",
        "            [46], # Harp\n",
        "            [56, 57, 58, 59, 60], # Trumpet\n",
        "            [64, 65, 66, 67, 68, 69, 70, 71], # Sax\n",
        "            [72, 73, 74, 75, 76, 77, 78], # Flute\n",
        "            [-1], # Drums\n",
        "            [52, 53], # Choir\n",
        "            [16, 17, 18, 19, 20] # Organ\n",
        "            ]\n",
        "\n",
        "while itrack < len(score):\n",
        "    for event in score[itrack]:         \n",
        "        if event[0] == 'note' or event[0] == 'patch_change':\n",
        "            events_matrix.append(event)\n",
        "    itrack += 1\n",
        "\n",
        "events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "events_matrix1 = []\n",
        "\n",
        "for event in events_matrix:\n",
        "        if event[0] == 'patch_change':\n",
        "            patches[event[2]] = event[3]\n",
        "\n",
        "        if event[0] == 'note':\n",
        "            event.extend([patches[event[3]]])\n",
        "            once = False\n",
        "            \n",
        "            for p in patch_map:\n",
        "                if event[6] in p and event[3] != 9: # Except the drums\n",
        "                    event[3] = patch_map.index(p)\n",
        "                    once = True\n",
        "                    \n",
        "            if not once and event[3] != 9: # Except the drums\n",
        "                event[3] = 15 # All other instruments/patches channel\n",
        "                event[5] = max(80, event[5])\n",
        "                \n",
        "            if event[3] < 12: # We won't write chans 12-16 for now...\n",
        "                events_matrix1.append(event)\n",
        "                \n",
        "#=======================================================\n",
        "# PRE-PROCESSING\n",
        "\n",
        "if len(events_matrix1) > 0:\n",
        "\n",
        "  # recalculating timings\n",
        "  for e in events_matrix1:\n",
        "      e[1] = int(e[1] / 8) # Max 2 seconds for start-times\n",
        "      e[2] = int(e[2] / 16) # Max 4 seconds for durations\n",
        "\n",
        "  # Sorting by pitch, then by start-time\n",
        "  events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "  events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "  #=======================================================\n",
        "  # FINAL PRE-PROCESSING\n",
        "\n",
        "  melody_chords = []\n",
        "\n",
        "  pe = events_matrix1[0]\n",
        "\n",
        "  for e in events_matrix1:\n",
        "    if e[1] >= 0 and e[2] >= 0:\n",
        "\n",
        "      # Cliping all values...\n",
        "      time = max(0, min(255, e[1]-pe[1]))             \n",
        "      dur = max(1, min(255, e[2]))\n",
        "      cha = max(0, min(11, e[3]))\n",
        "      ptc = max(1, min(127, e[4]))\n",
        "      vel = max(8, min(127, e[5]))\n",
        "\n",
        "      velocity = round(vel / 15)\n",
        "\n",
        "      # Writing final note \n",
        "      melody_chords.append([time, dur, cha, ptc, velocity])\n",
        "\n",
        "      pe = e\n",
        "\n",
        "\n",
        "  if len(melody_chords) > 1024:            \n",
        "\n",
        "    #=======================================================\n",
        "    # MAIN PROCESSING CYCLE\n",
        "    #=======================================================\n",
        "    \n",
        "    mel_cho = []\n",
        "\n",
        "    for m in melody_chords:\n",
        "        \n",
        "        # WRITING EACH NOTE HERE\n",
        "        time = m[0]\n",
        "        dur = m[1]\n",
        "        cha_vel = (m[2] * 8) + (m[4]-1)\n",
        "        cha_ptc = (m[2] * 128) + m[3]\n",
        "\n",
        "        mel_cho.extend([time, dur+256, cha_vel+512, cha_ptc+608])\n",
        "        \n",
        "    # TOTAL DICTIONARY SIZE 2144+1 = 2145\n",
        "\n",
        "    #=======================================================\n",
        "    # FINAL PROCESSING\n",
        "    #=======================================================\n",
        "\n",
        "# =================================\n",
        "\n",
        "out1 = mel_cho\n",
        "\n",
        "if len(out1) != 0:\n",
        "    \n",
        "    song = out1\n",
        "    song_f = []\n",
        "    time = 0\n",
        "    dur = 0\n",
        "    vel = 0\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "                    \n",
        "    for ss in song:\n",
        "      \n",
        "      if ss > 0 and ss < 256:\n",
        "\n",
        "          time += ss * 8\n",
        "        \n",
        "      if ss >= 256 and ss < 512:\n",
        "\n",
        "          dur = (ss-256) * 16\n",
        "\n",
        "      if ss >= 512 and ss < 608:\n",
        "\n",
        "          channel = (ss-512) // 8\n",
        "          vel = (((ss-512) % 8)+1) * 15\n",
        "              \n",
        "      if ss >= 608 and ss < 608+(12*128):\n",
        "          \n",
        "          pitch = (ss-608) % 128\n",
        "\n",
        "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                        output_signature = 'Perceiver Music Transformer',  \n",
        "                                                        output_file_name = '/content/Perceiver-Music-Transformer-Seed-MIDI', \n",
        "                                                        track_name='Project Los Angeles',\n",
        "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
        "                                                        number_of_ticks_per_quarter=500)\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "print('Displaying resulting composition...')\n",
        "fname = '/content/Perceiver-Music-Transformer-Seed-MIDI'\n",
        "\n",
        "x = []\n",
        "y =[]\n",
        "c = []\n",
        "\n",
        "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
        "\n",
        "for s in song_f:\n",
        "  x.append(s[1] / 1000)\n",
        "  y.append(s[4])\n",
        "  c.append(colors[s[3]])\n",
        "\n",
        "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "display(Audio(str(fname + '.wav'), rate=16000))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "ax=plt.axes(title=fname)\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "plt.scatter(x,y, c=c)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Pitch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dJaRwK9bUKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuation"
      ],
      "metadata": {
        "id": "aI0laUdWAkA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Single Continuation Block Generator\n",
        "\n",
        "number_of_prime_notes = 64 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "number_of_tokens_to_generate = 512 #@param {type:\"slider\", min:64, max:2048, step:32}\n",
        "number_of_batches_to_generate = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "return_prefix_and_prime_tokens = False #@param {type:\"boolean\"}\n",
        "\n",
        "#===================================================================\n",
        "print('=' * 70)\n",
        "print('Perceiver Music Transformer Model Continuation Generator')\n",
        "print('=' * 70)\n",
        "\n",
        "print('Generation settings:')\n",
        "print('=' * 70)\n",
        "print('Number of prime notes:', number_of_prime_notes)\n",
        "print('Number of tokens to generate:', number_of_tokens_to_generate)\n",
        "print('Number of batches to generate:', number_of_batches_to_generate)\n",
        "print('Model temperature:', temperature)\n",
        "\n",
        "print('=' * 70)\n",
        "\n",
        "num_toks = 4096 + (number_of_prime_notes * 4)\n",
        "\n",
        "inp = [mel_cho[:num_toks]] * number_of_batches_to_generate\n",
        "\n",
        "inp = torch.LongTensor(inp).cuda()\n",
        "\n",
        "out = model.module.generate(inp, \n",
        "                     number_of_tokens_to_generate, \n",
        "                     temperature=temperature,\n",
        "                     return_prime=return_prefix_and_prime_tokens)  \n",
        "\n",
        "out0 = out.tolist()\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "print('=' * 70)\n",
        "\n",
        "#======================================================================\n",
        "\n",
        "print('Rendering results...')\n",
        "print('=' * 70)\n",
        "\n",
        "for i in range(number_of_batches_to_generate):\n",
        "\n",
        "  print('=' * 70)\n",
        "  print('Batch #', i)\n",
        "  print('=' * 70)\n",
        "\n",
        "  out1 = out0[i]\n",
        "\n",
        "  print('Sample INTs', out1[:12])\n",
        "  print('=' * 70)\n",
        "\n",
        "  if len(out1) != 0:\n",
        "      \n",
        "      song = out1\n",
        "      song_f = []\n",
        "      time = 0\n",
        "      dur = 0\n",
        "      vel = 0\n",
        "      pitch = 0\n",
        "      channel = 0\n",
        "                      \n",
        "      for ss in song:\n",
        "        \n",
        "        if ss > 0 and ss < 256:\n",
        "\n",
        "            time += ss * 8\n",
        "          \n",
        "        if ss >= 256 and ss < 512:\n",
        "\n",
        "            dur = (ss-256) * 16\n",
        "\n",
        "        if ss >= 512 and ss < 608:\n",
        "\n",
        "            channel = (ss-512) // 8\n",
        "            vel = (((ss-512) % 8)+1) * 15\n",
        "                \n",
        "        if ss >= 608 and ss < 608+(12*128):\n",
        "            \n",
        "            pitch = (ss-608) % 128\n",
        "\n",
        "            song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "      detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                          output_signature = 'Perceiver Music Transformer',  \n",
        "                                                          output_file_name = '/content/Perceiver-Music-Transformer-Composition', \n",
        "                                                          track_name='Project Los Angeles',\n",
        "                                                          list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
        "                                                          number_of_ticks_per_quarter=500)\n",
        "\n",
        "      print('Done!')\n",
        "\n",
        "  print('Displaying resulting composition...')\n",
        "  fname = '/content/Perceiver-Music-Transformer-Composition'\n",
        "\n",
        "  x = []\n",
        "  y =[]\n",
        "  c = []\n",
        "\n",
        "  colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
        "\n",
        "  for s in song_f:\n",
        "    x.append(s[1] / 1000)\n",
        "    y.append(s[4])\n",
        "    c.append(colors[s[3]])\n",
        "\n",
        "  FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "  display(Audio(str(fname + '.wav'), rate=16000))\n",
        "\n",
        "  plt.figure(figsize=(14,5))\n",
        "  ax=plt.axes(title=fname)\n",
        "  ax.set_facecolor('black')\n",
        "\n",
        "  plt.scatter(x,y, c=c)\n",
        "  plt.xlabel(\"Time\")\n",
        "  plt.ylabel(\"Pitch\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bAWBH-MudV3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzCMd94Tu_gz"
      },
      "source": [
        "# Congrats! You did it! :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}