{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Perceiver Multi-instrumental Maker (Version 1.0)\n",
    "\n",
    "https://github.com/asigalov61/tegridy-tools\n",
    "\n",
    "https://github.com/lucidrains/perceiver-ar-pytorch\n",
    "\n",
    "## Project Los Angeles\n",
    "## Tegridy Code 2022\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L82QbwLTPclr"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/asigalov61/tegridy-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQwiG4rrUXba"
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install torch-summary\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L6S9kv7hlTr"
   },
   "outputs": [],
   "source": [
    "# Load modules and make data dir\n",
    "\n",
    "print('Loading modules...')\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import secrets\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from sklearn import metrics\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "%cd /notebooks/tegridy-tools/tegridy-tools/Perceiver-AR/\n",
    "\n",
    "from perceiver_ar_pytorch import PerceiverAR\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "%cd /notebooks/\n",
    "\n",
    "if not os.path.exists('/notebooks/INTS'):\n",
    "    os.makedirs('/notebooks/INTS')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unzip training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceiver Multi-Instrumental Training Data Pack\n",
    "%cd /notebooks/INTS/\n",
    "!wget --no-check-certificate -O 'Perceiver-MI-Training-Data.zip' \"https://onedrive.live.com/download?cid=8A0D502FC99C608F&resid=8A0D502FC99C608F%2118738&authkey=AF6k171kUSX-Yrk\"\n",
    "!unzip 'Perceiver-MI-Training-Data.zip'\n",
    "!rm 'Perceiver-MI-Training-Data.zip'\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "\n",
    "dataset_addr = \"/notebooks/INTS\"\n",
    "\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "filez.sort()\n",
    "\n",
    "print('Loading training data... Please wait...')\n",
    "\n",
    "train_data = torch.Tensor()\n",
    "\n",
    "for f in tqdm.tqdm(filez):\n",
    "    train_data = torch.cat((train_data, torch.Tensor(pickle.load(open(f, 'rb')))))\n",
    "    print('Loaded file:', f)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:15], train_data[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 8192 * 4 # 32k\n",
    "PREFIX_SEQ_LEN = (8192 * 4) - 1024\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "NUM_BATCHES = len(train_data) // SEQ_LEN // BATCH_SIZE\n",
    "\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 200\n",
    "SAVE_EVERY = 2000\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "print('Done!')\n",
    "      \n",
    "summary(model)\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # random sampling\n",
    "        # idx = secrets.randbelow((self.data.size(0) // (self.seq_len))-1) * (self.seq_len)\n",
    "        \n",
    "        # consequtive sampling seems to be better at 64k seq_len\n",
    "        idx = index * self.seq_len\n",
    "        \n",
    "        full_seq = self.data[idx: idx + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.size(0) // self.seq_len)-1\n",
    "\n",
    "train_dataset = MusicDataset(train_data, SEQ_LEN)\n",
    "val_dataset   = MusicDataset(train_data, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BoaqJAOU5M8"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QauuLTxFTDXw"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='Training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss, acc = model(next(train_loader))\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'Training loss: {loss.item()}')\n",
    "    print(f'Training acc: {acc.item()}')\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_accs.append(acc.item())\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = model(next(val_loader))\n",
    "            print(f'Validation loss: {val_loss.item()}')\n",
    "            print(f'Validation acc: {val_acc.item()}')\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "            \n",
    "            print('Plotting training loss graph...')\n",
    "            \n",
    "            tr_loss_list = train_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/training_loss_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting training acc graph...')\n",
    "            \n",
    "            tr_loss_list = train_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/training_acc_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation loss graph...')\n",
    "            tr_loss_list = val_losses\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "            \n",
    "            print('Plotting validation acc graph...')\n",
    "            tr_loss_list = val_accs\n",
    "            plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "            plt.show()\n",
    "            # plt.savefig('/notebooks/validation_accs_graph.png')\n",
    "            plt.close()\n",
    "            print('Done!')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        \n",
    "        print(inp)\n",
    "\n",
    "        sample = model.generate(inp[None, ...], GENERATE_LENGTH)\n",
    "        \n",
    "        print(sample)\n",
    "        \n",
    "    if i % SAVE_EVERY == 0:\n",
    "        \n",
    "        print('Saving model progress. Please wait...')\n",
    "        print('model_checkpoint_' + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth')\n",
    "        \n",
    "        fname = '/notebooks/model_checkpoint_'  + str(i) + '_steps_' + str(round(float(train_losses[-1]), 4)) + '_loss.pth'\n",
    "        \n",
    "        torch.save(model.state_dict(), fname)\n",
    "        \n",
    "        print('Done!')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save stats graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(train_losses))] ,train_losses, 'b')\n",
    "plt.savefig('/notebooks/training_loss_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save training acc graph\n",
    "\n",
    "plt.plot([i for i in range(len(train_accs))] ,train_accs, 'b')\n",
    "plt.savefig('/notebooks/training_acc_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save validation loss graph\n",
    "\n",
    "plt.plot([i for i in range(len(val_losses))] ,val_losses, 'b')\n",
    "plt.savefig('/notebooks/validation_loss_graph.png')\n",
    "plt.close()\n",
    "print('Done!')\n",
    "\n",
    "# Save validation acc graph\n",
    "\n",
    "plt.plot([i for i in range(len(val_accs))] ,val_accs, 'b')\n",
    "plt.savefig('/notebooks/validation_acc_graph.png')\n",
    "plt.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load/Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 8192 * 4 # 32k\n",
    "PREFIX_SEQ_LEN = (8192 * 4) - 1024\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load('model_checkpoint_0_steps_6.8286_loss.pth')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Model stats\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Embeddings\n",
    "\n",
    "cos_sim = metrics.pairwise.cosine_similarity(\n",
    "   model.net.token_emb.weight.detach().cpu().numpy()[0].reshape(-1, 1)\n",
    ")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"none\")\n",
    "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
    "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "plt.savefig(\"/notebooks/Perceiver-Positional-Embeddings-Plot.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MIDI option\n",
    "\n",
    "full_path_to_custom_MIDI_file = \"/notebooks/tegridy-tools/tegridy-tools/seed2.mid\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading custom MIDI file...')\n",
    "score = TMIDIX.midi2ms_score(open(full_path_to_custom_MIDI_file, 'rb').read())\n",
    "\n",
    "events_matrix = []\n",
    "\n",
    "itrack = 1\n",
    "\n",
    "patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "patch_map = [[0, 1, 2, 3, 4, 5, 6, 7], # Piano \n",
    "              [24, 25, 26, 27, 28, 29, 30], # Guitar\n",
    "              [32, 33, 34, 35, 36, 37, 38, 39], # Bass\n",
    "              [40, 41], # Violin\n",
    "              [42, 43], # Cello\n",
    "              [46], # Harp\n",
    "              [56, 57, 58, 59, 60], # Trumpet\n",
    "              [71, 72], # Clarinet\n",
    "              [73, 74, 75], # Flute\n",
    "              [-1], # Fake Drums\n",
    "              [52, 53], # Choir\n",
    "              [16, 17, 18, 19, 20] # Organ\n",
    "            ]\n",
    "\n",
    "while itrack < len(score):\n",
    "    for event in score[itrack]:         \n",
    "        if event[0] == 'note' or event[0] == 'patch_change':\n",
    "            events_matrix.append(event)\n",
    "    itrack += 1\n",
    "\n",
    "events_matrix.sort(key=lambda x: x[1])\n",
    "\n",
    "events_matrix1 = []\n",
    "for event in events_matrix:\n",
    "        if event[0] == 'patch_change':\n",
    "            patches[event[2]] = event[3]\n",
    "\n",
    "        if event[0] == 'note':\n",
    "            event.extend([patches[event[3]]])\n",
    "            once = False\n",
    "            \n",
    "            for p in patch_map:\n",
    "                if event[6] in p and event[3] != 9: # Except the drums\n",
    "                    event[3] = patch_map.index(p)\n",
    "                    once = True\n",
    "                    \n",
    "            if not once and event[3] != 9: # Except the drums\n",
    "                event[3] = 0 # All other instruments/patches channel\n",
    "                event[5] = max(80, event[5])\n",
    "                \n",
    "            if event[3] < 12: # We won't write chans 11-16 for now...\n",
    "                events_matrix1.append(event)\n",
    "\n",
    "# Sorting...\n",
    "events_matrix1.sort(key=lambda x: (x[1], x[3]))\n",
    "\n",
    "# recalculating timings\n",
    "for e in events_matrix1:\n",
    "    e[1] = int(e[1] / 16)\n",
    "    e[2] = int(e[2] / 32)\n",
    "\n",
    "# final processing...\n",
    "\n",
    "inputs = []\n",
    "\n",
    "melody = []\n",
    "\n",
    "melody_chords = []\n",
    "\n",
    "pe = events_matrix1[0]\n",
    "for e in events_matrix1:\n",
    "\n",
    "    time = max(0, min(127, e[1]-pe[1]))\n",
    "    dur = max(1, min(127, e[2]))\n",
    "    cha = max(0, min(11, e[3]))\n",
    "    ptc = max(1, min(127, e[4]))\n",
    "    vel = max(19, min(127, e[5]))\n",
    "\n",
    "    div_vel = int(vel / 19)\n",
    "\n",
    "    chan_vel = (cha * 11) + div_vel\n",
    "\n",
    "    # Continuation / Inpainting\n",
    "    inputs.extend([chan_vel, time+128, dur+256, ptc+384])\n",
    "\n",
    "    # Melody Orchestration\n",
    "    if time != 0:\n",
    "      if ptc < 60:\n",
    "        ptc = (ptc % 12) + 60  \n",
    "\n",
    "      # Converted to Piano\n",
    "      melody.extend([div_vel, time+128, dur+256, ptc+384])\n",
    "\n",
    "    # For future development\n",
    "    melody_chords.append([time, dur, cha, ptc, vel])\n",
    "\n",
    "    pe = e\n",
    "\n",
    "# =================================\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Nh0Vw6hqhi"
   },
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "inp = val_dataset[0]\n",
    "\n",
    "inp = [0, 127+128, 127+256, 0+384] * ((8192 * 3) + (8192-1024))\n",
    "\n",
    "inp += inputs[:1024]\n",
    "\n",
    "inp = torch.LongTensor(inp).cuda()\n",
    "\n",
    "print(inp)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out = model.generate(inp[None, ...], \n",
    "                     1024, \n",
    "                     temperature=0.6)\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evZLAi-8e_Ok"
   },
   "outputs": [],
   "source": [
    "# Convert to MIDI\n",
    "\n",
    "out1 = out.cpu().tolist()[0]\n",
    "\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = s[0] // 11\n",
    "\n",
    "        vel = (s[0] % 11) * 19\n",
    "\n",
    "        time += (s[1]-128) * 16\n",
    "            \n",
    "        dur = (s[2] - 256) * 32\n",
    "        \n",
    "        pitch = (s[3] - 384)\n",
    "                                  \n",
    "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = '/notebooks/Perceiver-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
